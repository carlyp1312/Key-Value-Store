Sharding: Shard to node mapping. Iterate through a loop and assigning a node to a shard. Once the counter hits the end of the shardlist, start assining the node from the beginning of the shard list. Key to shard mapping was used using the hashlib library in order to ensure that hashes were consistent.

Causal Dependency: In order to ensure the system was causally dependent, we used vector clock implementation. Before any operation to the key value store endpoint, we ensured that the metadata (vector clock) was less than or equal to the local vector clock of the receiving replica. A vector clock's position is only incremented for sends to another replica (broadcasting), so whenever a replica received a PUT/DELETE request, it would first increment its own position in the vector clock and then broadcast the request to the running replicas.

Replica Detection: Whenever a replica broadcasts a write operation to other replicas, the request was put into a try except block. If the request led to a connection error, we knew that the replica was down (either disconnected or killed). In order to ensure replication, whenever the replica was reconnected to the system, we updated its key value store and causal metadata store by copying them over from another replica's.
